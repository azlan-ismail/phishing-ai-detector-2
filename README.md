
# ğŸ§ª Hands-on 2: Adversarial Attack Evaluation & Explainability

## ğŸ¯ Objective
This tutorial focuses on evaluating the robustness of phishing detection models against adversarial attacks and interpreting their decisions using SHAP and LIME.

---

## ğŸ“ Project Folder Structure

```
phishing-ai-detector/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ phishing.csv
â”‚   â”œâ”€â”€ X_adv_lr.csv
â”‚   â”œâ”€â”€ X_adv_rf.csv
â”‚   â””â”€â”€ X_adv_xgb.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ phishing_model_lr.pkl
â”‚   â”œâ”€â”€ phishing_model_rf.pkl
â”‚   â””â”€â”€ phishing_model_xgb.pkl
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ adversarial_metrics.json
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Adversarial_Attack_Evaluation.ipynb
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_adversarial_eval.py
```

---

## ğŸ§  Step-by-Step Instructions

### Step 1: Load Original Dataset and Trained Models
- Dataset: `data/phishing.csv`
- Models: Logistic Regression, Random Forest, XGBoost (`models/phishing_model_*.pkl`)

### Step 2: Generate Adversarial Examples
Use the `run_adversarial_eval.py` script to create and evaluate adversarial samples:

```bash
python scripts/run_adversarial_eval.py
```

- This script loads the original models and generates adversarial examples using `BoundaryAttack`.
- Output CSVs will be saved as:
  - `data/X_adv_lr.csv`
  - `data/X_adv_rf.csv`
  - `data/X_adv_xgb.csv`
- Evaluation metrics saved to `metrics/adversarial_metrics.json`

### Step 3: Open the Notebook
Navigate to `notebooks/Adversarial_Attack_Evaluation.ipynb`. Run all cells.

It includes:
- Accuracy evaluation on adversarial inputs
- SHAP explanations (for all three models)
- LIME explanations (for all three models)

### Step 4: SHAP Explanations
- Use `shap.Explainer(model.predict_proba, X)`
- Show individual predictions using `shap.plots.waterfall(...)`
- Understand **which features contributed most** to misclassifications

### Step 5: LIME Explanations
- Use `LimeTabularExplainer(...)` for black-box interpretability
- Explain the same adversarial samples
- View local decision boundaries via `show_in_notebook(...)`

---

## ğŸ“ˆ Expected Outcomes

| Model              | Accuracy on Clean Data | Accuracy on Adversarial Data |
|--------------------|------------------------|-------------------------------|
| Logistic Regression | ~95%                   | â†“ after attack                |
| Random Forest       | ~98%                   | â†“ after attack                |
| XGBoost             | ~99%                   | â†“ after attack                |

You should observe **performance drops**, and use SHAP/LIME to diagnose **why the models failed**.

---

## ğŸ“ Deliverables

1. Completed notebook: `Adversarial_Attack_Evaluation.ipynb`
2. CSVs for adversarial test sets
3. Summary of attack results and interpretations (optional slide/report)
